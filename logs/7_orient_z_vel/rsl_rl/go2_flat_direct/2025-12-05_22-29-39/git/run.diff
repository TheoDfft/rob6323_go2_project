--- git status ---
On branch master
Your branch is up to date with 'origin/master'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   scripts/rsl_rl/__pycache__/cli_args.cpython-311.pyc
	modified:   source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env.py
	modified:   source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env_cfg.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/scripts/rsl_rl/__pycache__/cli_args.cpython-311.pyc b/scripts/rsl_rl/__pycache__/cli_args.cpython-311.pyc
index cfe10a7..f2cdd8a 100644
Binary files a/scripts/rsl_rl/__pycache__/cli_args.cpython-311.pyc and b/scripts/rsl_rl/__pycache__/cli_args.cpython-311.pyc differ
diff --git a/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env.py b/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env.py
index 18baa94..193f994 100644
--- a/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env.py
+++ b/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env.py
@@ -43,11 +43,12 @@ class Rob6323Go2Env(DirectRLEnv):
         self._episode_sums = {
             key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
             for key in [
-                "track_lin_vel_xy_exp",
-                "track_ang_vel_z_exp",
-                "rew_action_rate",
-                "raibert_heuristic",
+                "track_lin_vel_xy_exp",  # === ADDED: Part 1 === 
+                "track_ang_vel_z_exp",  # === ADDED: Part 2 ===
+                "rew_action_rate",  # === ADDED: Part 3 ===
+                "raibert_heuristic",  # === ADDED: Part 4 ===
                 "orient",  # === ADDED: Part 5 ===
+                "lin_vel_z",  # === ADDED: Penalize vertical bouncing ===
             ]
         }
 
@@ -80,8 +81,6 @@ class Rob6323Go2Env(DirectRLEnv):
         self.clock_inputs = torch.zeros(self.num_envs, 4, dtype=torch.float, device=self.device, requires_grad=False)
         self.desired_contact_states = torch.zeros(self.num_envs, 4, dtype=torch.float, device=self.device, requires_grad=False)
 
-    
-
     def _setup_scene(self):
         self.robot = Articulation(self.cfg.robot_cfg)
         self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
@@ -168,13 +167,17 @@ class Rob6323Go2Env(DirectRLEnv):
         # === ADDED: Part 5 - Orientation penalty ===
         # Penalize non-flat orientation (projected gravity XY should be 0 when robot is flat)
         rew_orient = torch.sum(torch.square(self.robot.data.projected_gravity_b[:, :2]), dim=1)
+
+        # === ADDED: Part Penalize vertical velocity (z-component of base linear velocity) ===
+        rew_lin_vel_z = torch.square(self.robot.data.root_lin_vel_b[:, 2])
         
         rewards = {
             "track_lin_vel_xy_exp": lin_vel_error_mapped * self.cfg.lin_vel_reward_scale,
             "track_ang_vel_z_exp": yaw_rate_error_mapped * self.cfg.yaw_rate_reward_scale,
             "rew_action_rate": rew_action_rate * self.cfg.action_rate_reward_scale,
             "raibert_heuristic": rew_raibert_heuristic * self.cfg.raibert_heuristic_reward_scale,
-            "orient": rew_orient * self.cfg.orient_reward_scale,  # === ADDED ===
+            "orient": rew_orient * self.cfg.orient_reward_scale,
+            "lin_vel_z": rew_lin_vel_z * self.cfg.lin_vel_z_reward_scale,
         }
         reward = torch.sum(torch.stack(list(rewards.values())), dim=0)
         # Logging
@@ -358,8 +361,7 @@ class Rob6323Go2Env(DirectRLEnv):
         cur_footsteps_translated = self.foot_positions_w - self.robot.data.root_pos_w.unsqueeze(1)
         footsteps_in_body_frame = torch.zeros(self.num_envs, 4, 3, device=self.device)
         for i in range(4):
-            footsteps_in_body_frame[:, i, :] = math_utils.quat_apply_yaw(math_utils.quat_conjugate(self.robot.data.root_quat_w),
-                                                            cur_footsteps_translated[:, i, :])
+            footsteps_in_body_frame[:, i, :] = math_utils.quat_apply_yaw(math_utils.quat_conjugate(self.robot.data.root_quat_w), cur_footsteps_translated[:, i, :])
 
         # nominal positions: [FR, FL, RR, RL]
         desired_stance_width = 0.25
diff --git a/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env_cfg.py b/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env_cfg.py
index b146ad9..0f6c407 100644
--- a/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env_cfg.py
+++ b/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env_cfg.py
@@ -33,7 +33,7 @@ class Rob6323Go2EnvCfg(DirectRLEnvCfg):
     state_space = 0
     debug_vis = True
     # === ADDED Part 3: Early termination ===
-    base_height_min = 0.15  # Terminate if base is lower than 15cm
+    base_height_min = 0.20  # Terminate if base is lower than 20cm
 
     # simulation
     sim: SimulationCfg = SimulationCfg(
@@ -111,4 +111,5 @@ class Rob6323Go2EnvCfg(DirectRLEnvCfg):
     tracking_contacts_shaped_force_reward_scale = 4.0
 
     # === ADDED: Part 5 rewards ===
-    orient_reward_scale = -5.0  # Penalize non-flat orientation
+    orient_reward_scale = -0.5  # Penalize non-flat orientation
+    lin_vel_z_reward_scale = -5.0  # Penalize vertical bouncing