--- git status ---
On branch master
Your branch is up to date with 'origin/master'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   scripts/rsl_rl/__pycache__/cli_args.cpython-311.pyc
	modified:   source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env.py
	modified:   source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env_cfg.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/BACKUP_rob6323_go2_env.py
	source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/BACKUP_rob6323_go2_env_cfg.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/scripts/rsl_rl/__pycache__/cli_args.cpython-311.pyc b/scripts/rsl_rl/__pycache__/cli_args.cpython-311.pyc
index cfe10a7..f2cdd8a 100644
Binary files a/scripts/rsl_rl/__pycache__/cli_args.cpython-311.pyc and b/scripts/rsl_rl/__pycache__/cli_args.cpython-311.pyc differ
diff --git a/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env.py b/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env.py
index 84df2db..9292152 100644
--- a/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env.py
+++ b/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env.py
@@ -2,12 +2,13 @@
 # All rights reserved.
 #
 # SPDX-License-Identifier: BSD-3-Clause
+#
+# BIPEDAL STANCE IMPLEMENTATION
+# Reference: go2_bipedal.py line 1522 (reward), lines 1894-1901 (terminations)
 
 from __future__ import annotations
 
-
 import gymnasium as gym
-import math
 import torch
 import numpy as np
 from collections.abc import Sequence
@@ -15,8 +16,6 @@ from collections.abc import Sequence
 import isaaclab.sim as sim_utils
 from isaaclab.assets import Articulation
 from isaaclab.envs import DirectRLEnv
-from isaaclab.sim.spawners.from_files import GroundPlaneCfg, spawn_ground_plane
-from isaaclab.utils.math import sample_uniform
 from isaaclab.sensors import ContactSensor
 from isaaclab.markers import VisualizationMarkers
 import isaaclab.utils.math as math_utils
@@ -36,61 +35,94 @@ class Rob6323Go2Env(DirectRLEnv):
             self.num_envs, gym.spaces.flatdim(self.single_action_space), device=self.device
         )
 
-        # X/Y linear velocity and yaw angular velocity commands
-        self._commands = torch.zeros(self.num_envs, 3, device=self.device)
+        # === BIPEDAL: Key vectors for upright posture ===
+        # forward_vec: robot's forward direction in body frame
+        self.forward_vec = torch.tensor([1.0, 0.0, 0.0], device=self.device).repeat(self.num_envs, 1)
+        # upright_vec: target upright direction (slightly tilted forward)
+        self.upright_vec = torch.tensor([0.2, 0.0, -1.0], device=self.device).repeat(self.num_envs, 1)
 
-        # Logging
+        # === BIPEDAL: Front/Rear feet indices ===
+        self.fidx = [0, 1]      # Front feet: FL, FR
+        self.rear_fidx = [2, 3]  # Rear feet: RL, RR
+
+        # Logging - bipedal reward terms
         self._episode_sums = {
             key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
             for key in [
-                "track_lin_vel_xy_exp",  # === ADDED: Part 1 === 
-                "track_ang_vel_z_exp",  # === ADDED: Part 2 ===
-                "rew_action_rate",  # === ADDED: Part 3 ===
-                "raibert_heuristic",  # === ADDED: Part 4 ===
-                "orient",  # === ADDED: Part 5 ===
-                "lin_vel_z",  # === ADDED: Penalize vertical bouncing ===
-                "dof_vel",  # === ADDED: Penalize high joint velocities ===
-                "ang_vel_xy",  # === ADDED: Penalize roll/pitch angular velocity ===
-                "feet_clearance",  # === ADDED: Penalize low foot height during swing ===
-                "tracking_contacts_shaped_force",  # === ADDED: Match gait contact plan ===
+                "upright",
+                "lift_up", 
+                "stand_air",
+                "lin_vel_xy",
+                "feet_clearance",
+                "tracking_contacts",
+                "torque_limits",
+                "ang_vel_z",
+                "action_rate",
+                "dof_vel",
             ]
         }
 
-        # === ADDED Part 1: Action history for rate penalization ===
-        self.last_actions = torch.zeros(self.num_envs, gym.spaces.flatdim(self.single_action_space), 3, dtype=torch.float, device=self.device, requires_grad=False)
+        # Action history for rate penalization (keep 3 frames: current, t-1, t-2)
+        self.last_actions = torch.zeros(
+            self.num_envs, gym.spaces.flatdim(self.single_action_space), 3, 
+            dtype=torch.float, device=self.device, requires_grad=False
+        )
+
+        # === BIPEDAL: DOF position history for abrupt_change_condition ===
+        self.last_dof_pos = torch.zeros(
+            self.num_envs, 12, 1,  # Just need previous frame
+            dtype=torch.float, device=self.device, requires_grad=False
+        )
+
+        # === BIPEDAL: Store computed torques for torque_limits reward ===
+        self.computed_torques = torch.zeros(self.num_envs, 12, device=self.device)
 
         # Get specific body indices
         self._base_id, _ = self._contact_sensor.find_bodies("base")
-        # self._feet_ids, _ = self._contact_sensor.find_bodies(".*foot")
-        # self._undesired_contact_body_ids, _ = self._contact_sensor.find_bodies(".*thigh")
 
-        # add handle for debug visualization (this is set to a valid handle inside set_debug_vis)
+        # add handle for debug visualization
         self.set_debug_vis(self.cfg.debug_vis)
 
-        # === ADDED Part 2: PD control parameters ===
+        # PD control parameters
         self.Kp = torch.tensor([cfg.Kp] * 12, device=self.device).unsqueeze(0).repeat(self.num_envs, 1)
         self.Kd = torch.tensor([cfg.Kd] * 12, device=self.device).unsqueeze(0).repeat(self.num_envs, 1)
         self.motor_offsets = torch.zeros(self.num_envs, 12, device=self.device)
-        self.torque_limits = cfg.torque_limits
+        self.torque_limits_value = cfg.torque_limits
 
-        # === ADDED Part 4: Feet body indices for Raibert heuristic ===
+        # Feet body indices
         self._feet_ids = []
         foot_names = ["FL_foot", "FR_foot", "RL_foot", "RR_foot"]
         for name in foot_names:
             id_list, _ = self.robot.find_bodies(name)
             self._feet_ids.append(id_list[0])
 
-        # === ADDED Part 6: Feet indices in CONTACT SENSOR (different from robot body indices!) ===
+        # Feet indices in CONTACT SENSOR (different from robot body indices!)
         self._feet_ids_sensor = []
         for name in foot_names:
             id_list, _ = self._contact_sensor.find_bodies(name)
             self._feet_ids_sensor.append(id_list[0])
 
-        # === ADDED Part 4: Gait variables for Raibert heuristic ===
+        # Gait variables (for front feet only in bipedal)
         self.gait_indices = torch.zeros(self.num_envs, dtype=torch.float, device=self.device, requires_grad=False)
         self.clock_inputs = torch.zeros(self.num_envs, 4, dtype=torch.float, device=self.device, requires_grad=False)
         self.desired_contact_states = torch.zeros(self.num_envs, 4, dtype=torch.float, device=self.device, requires_grad=False)
 
+        # === BIPEDAL: Joint position limits for position_protect termination ===
+        # Go2 joint limits (approximate values in radians)
+        self.dof_pos_limits = torch.zeros(12, 2, device=self.device)
+        # Hip joints: [-1.047, 1.047] rad
+        self.dof_pos_limits[[0, 3, 6, 9], 0] = -1.047
+        self.dof_pos_limits[[0, 3, 6, 9], 1] = 1.047
+        # Thigh joints: [-1.5, 3.4] rad
+        self.dof_pos_limits[[1, 4, 7, 10], 0] = -1.5
+        self.dof_pos_limits[[1, 4, 7, 10], 1] = 3.4
+        # Calf joints: [-2.7, -0.83] rad
+        self.dof_pos_limits[[2, 5, 8, 11], 0] = -2.7
+        self.dof_pos_limits[[2, 5, 8, 11], 1] = -0.83
+
+        # === BIPEDAL: Max DOF change threshold for abrupt_change_condition ===
+        self.max_dof_change = 0.75  # radians
+
     def _setup_scene(self):
         self.robot = Articulation(self.cfg.robot_cfg)
         self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
@@ -100,7 +132,6 @@ class Rob6323Go2Env(DirectRLEnv):
         self._terrain = self.cfg.terrain.class_type(self.cfg.terrain)
         # clone and replicate
         self.scene.clone_environments(copy_from_source=False)
-        # we need to explicitly filter collisions for CPU simulation
         if self.device == "cpu":
             self.scene.filter_collisions(global_prim_paths=[])
         # add articulation to scene
@@ -109,7 +140,6 @@ class Rob6323Go2Env(DirectRLEnv):
         light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))
         light_cfg.func("/World/Light", light_cfg)
 
-    # === MODIFIED Part 2: Custom PD controller ===
     def _pre_physics_step(self, actions: torch.Tensor) -> None:
         self._actions = actions.clone()
         # Compute desired joint positions from policy actions
@@ -120,33 +150,33 @@ class Rob6323Go2Env(DirectRLEnv):
 
     def _apply_action(self) -> None:
         # Compute PD torques
-        torques = torch.clip(
+        self.computed_torques = torch.clip(
             (
                 self.Kp * (self.desired_joint_pos - self.robot.data.joint_pos)
                 - self.Kd * self.robot.data.joint_vel
             ),
-            -self.torque_limits,
-            self.torque_limits,
+            -self.torque_limits_value,
+            self.torque_limits_value,
         )
         # Apply torques to the robot
-        self.robot.set_joint_effort_target(torques)
+        self.robot.set_joint_effort_target(self.computed_torques)
 
     def _get_observations(self) -> dict:
         self._previous_actions = self._actions.clone()
+        
+        # === BIPEDAL: Compute yaw-rotated upright vector (replaces commands) ===
+        upright_vec_obs = math_utils.quat_apply_yaw(self.robot.data.root_quat_w, self.upright_vec)
+        
         obs = torch.cat(
             [
-                tensor
-                for tensor in (
-                    self.robot.data.root_lin_vel_b,
-                    self.robot.data.root_ang_vel_b,
-                    self.robot.data.projected_gravity_b,
-                    self._commands,
-                    self.robot.data.joint_pos - self.robot.data.default_joint_pos,
-                    self.robot.data.joint_vel,
-                    self._actions,
-                    self.clock_inputs  # === ADDED Part 4: Gait phase info ===
-                )
-                if tensor is not None
+                self.robot.data.root_lin_vel_b,           # (3,) base linear velocity
+                self.robot.data.root_ang_vel_b,           # (3,) base angular velocity
+                self.robot.data.projected_gravity_b,       # (3,) projected gravity
+                upright_vec_obs,                           # (3,) yaw-rotated upright vector (replaces commands)
+                self.robot.data.joint_pos - self.robot.data.default_joint_pos,  # (12,) joint positions
+                self.robot.data.joint_vel,                 # (12,) joint velocities
+                self._actions,                             # (12,) previous actions
+                self.clock_inputs,                         # (4,) gait phase info
             ],
             dim=-1,
         )
@@ -154,93 +184,203 @@ class Rob6323Go2Env(DirectRLEnv):
         return observations
 
     def _get_rewards(self) -> torch.Tensor:
-        # linear velocity tracking
-        lin_vel_error = torch.sum(torch.square(self._commands[:, :2] - self.robot.data.root_lin_vel_b[:, :2]), dim=1)
-        lin_vel_error_mapped = torch.exp(-lin_vel_error / 0.25)
-        # yaw rate tracking
-        yaw_rate_error = torch.square(self._commands[:, 2] - self.robot.data.root_ang_vel_b[:, 2])
-        yaw_rate_error_mapped = torch.exp(-yaw_rate_error / 0.25)
-
-        # === ADDED Part 1: Action rate penalization ===
-        # First derivative (Current - Last)
-        rew_action_rate = torch.sum(torch.square(self._actions - self.last_actions[:, :, 0]), dim=1) * (self.cfg.action_scale ** 2)
-        # Second derivative (Current - 2*Last + 2ndLast)
-        rew_action_rate += torch.sum(torch.square(self._actions - 2 * self.last_actions[:, :, 0] + self.last_actions[:, :, 1]), dim=1) * (self.cfg.action_scale ** 2)
-        # Update the prev action hist (roll buffer and insert new action)
-        self.last_actions = torch.roll(self.last_actions, 1, 2)
-        self.last_actions[:, :, 0] = self._actions[:]
-
-        # === ADDED Part 4: Raibert heuristic ===
+        """Compute bipedal standing rewards.
+        Reference: go2_bipedal.py line 1522
+        rew_buf = rew_upright + rew_lift_up + rew_stand_air + rew_lin_vel_xy + 
+                  rew_feet_clearance + rew_tracking_contacts_shaped_force + 
+                  rew_torque_limits + rew_ang_vel_z + rew_action_rate + rew_dof_vel
+        """
+        
+        # === Phase conditions ===
+        allow_contact_steps = self.cfg.allow_contact_steps
+        condition_tc = self.episode_length_buf > allow_contact_steps
+        condition_early = ~condition_tc  # t <= t_c
+        
+        # === Compute is_stand indicator ===
+        forward = math_utils.quat_rotate(self.robot.data.root_quat_w, self.forward_vec)
+        upright_vec = math_utils.quat_apply_yaw(self.robot.data.root_quat_w, self.upright_vec)
+        cosine_dist = torch.sum(forward * upright_vec, dim=-1) / (torch.norm(upright_vec, dim=-1) + 1e-6)
+        is_stand = (cosine_dist > 0.9).float()
+        
+        # === Height scaling factor ===
+        root_height = self.robot.data.root_pos_w[:, 2]
+        height_scale = torch.clamp(
+            (root_height - self.cfg.scale_factor_low) / (self.cfg.scale_factor_high - self.cfg.scale_factor_low),
+            0.0, 1.0
+        )
+        
+        # === Update gait/contact states ===
         self._step_contact_targets()
-        rew_raibert_heuristic = self._reward_raibert_heuristic()
-
-        # === ADDED: Part 5 - Orientation penalty ===
-        # Penalize non-flat orientation (projected gravity XY should be 0 when robot is flat)
-        rew_orient = torch.sum(torch.square(self.robot.data.projected_gravity_b[:, :2]), dim=1)
-
-        # === ADDED: Part Penalize vertical velocity (z-component of base linear velocity) ===
-        rew_lin_vel_z = torch.square(self.robot.data.root_lin_vel_b[:, 2])
-
-        # === ADDED: Penalize high joint velocities ===
-        rew_dof_vel = torch.sum(torch.square(self.robot.data.joint_vel), dim=1)
-
-        # === ADDED: Penalize angular velocity in XY plane (roll/pitch) ===
-        rew_ang_vel_xy = torch.sum(torch.square(self.robot.data.root_ang_vel_b[:, :2]), dim=1)
-
-        # === ADDED: Penalize low foot height during swing phase ===
-        # Matches IsaacGym reference: reference/go2_terrain.py compute_reward_CaT()
-        # phases: 0 at start/end of swing, 1 at apex of swing
-        phases = 1 - torch.abs(1.0 - torch.clip((self.foot_indices * 2.0) - 1.0, 0.0, 1.0) * 2.0)
-        # Get foot heights (Z coordinate in world frame)
+        
+        # =====================================================================
+        # REWARD TERMS (from line 1522)
+        # =====================================================================
+        
+        # === 1. rew_upright: Upright posture reward ===
+        # k_up * (0.5 * cosine_dist + 0.5)^2
+        rew_upright = torch.square(0.5 * cosine_dist + 0.5) * self.cfg.upright_reward_scale
+        
+        # === 2. rew_lift_up: Height reward ===
+        # k_lift * clip((H - H_min) / (H_max - H_min), 0, 1)
+        lift_up_ratio = (root_height - self.cfg.lift_up_h_min) / (self.cfg.lift_up_h_max - self.cfg.lift_up_h_min)
+        rew_lift_up = torch.clamp(lift_up_ratio, 0.0, 1.0) * self.cfg.lift_up_reward_scale
+        
+        # === 3. rew_stand_air: Early phase stand-air penalty/reward ===
+        # Penalty: -k_air_pen * Σ_{front} max(0, h_f - 0.06) when t < t_c
+        # Reward: k_air_rew * Σ_{rear} min(h_f, 0.06) when t < t_c
         foot_heights = self.foot_positions_w[:, :, 2]
-        # Target height: 8cm max clearance at swing apex + 2cm foot radius offset
-        target_height = 0.08 * phases + 0.02
-        # Penalize deviation from target, only during swing (when desired_contact_states is 0)
-        rew_foot_clearance = torch.square(target_height - foot_heights) * (1 - self.desired_contact_states)
-        rew_feet_clearance = torch.sum(rew_foot_clearance, dim=1)
-
-        # === ADDED Part 6: Tracking contacts shaped force ===
-        # Matches IsaacGym reference: reference/go2_terrain.py compute_reward_CaT()
-        # Penalize contact forces during swing phase (when foot should be in air)
+        front_heights = foot_heights[:, self.fidx]  # Front feet heights
+        rear_heights = foot_heights[:, self.rear_fidx]  # Rear feet heights
+        
+        stand_air_penalty = torch.sum((front_heights - 0.06).clamp(min=0.0), dim=1) * self.cfg.stand_air_penalty_scale
+        stand_air_reward = torch.sum(rear_heights.clamp(max=0.06), dim=1) * self.cfg.stand_air_reward_scale
+        rew_stand_air = (stand_air_penalty + stand_air_reward) * condition_early.float()
+        
+        # === 4. rew_lin_vel_xy: No velocity reward + velocity penalty ===
+        # Reward: k_lin * exp(-||v_xy||^2 / δ) * is_stand * height_scale
+        # Penalty: -k_vel_pen * ||v_xy||^2 * 1_{t>t_c}
+        actual_lin_vel = self.robot.data.root_lin_vel_b[:, :2]
+        lin_vel_sq = torch.sum(torch.square(actual_lin_vel), dim=1)
+        
+        # No velocity reward (when standing)
+        lin_vel_reward = torch.exp(-lin_vel_sq / self.cfg.lin_vel_delta) * is_stand * height_scale * self.cfg.lin_vel_reward_scale
+        # Velocity penalty (after t_c)
+        vel_penalty = lin_vel_sq * condition_tc.float() * self.cfg.vel_penalty_scale
+        rew_lin_vel_xy = lin_vel_reward + vel_penalty
+        
+        # === 5. rew_feet_clearance: Front feet clearance (after t_c) ===
+        # -k_clr * 1_{t>t_c} * Σ_{front} (h_f - h_target)^2 * (1 - C_f)
+        phases = 1 - torch.abs(1.0 - torch.clip((self.foot_indices[:, self.fidx] * 2.0) - 1.0, 0.0, 1.0) * 2.0)
+        target_height = 0.08 * phases + 0.02  # 8cm max + 2cm foot radius
+        clearance_error = torch.square(target_height - front_heights) * (1 - self.desired_contact_states[:, self.fidx])
+        rew_feet_clearance = torch.sum(clearance_error, dim=1) * condition_tc.float() * self.cfg.feet_clearance_reward_scale
+        
+        # === 6. rew_tracking_contacts_shaped_force: Contact tracking (front feet, when standing) ===
+        # -k_cfs * is_stand * Σ_{front} (1 - C_f) * (1 - exp(-||f_f||^2 / 100))
         foot_forces = torch.norm(self._contact_sensor.data.net_forces_w[:, self._feet_ids_sensor, :], dim=-1)
-        desired_contact = self.desired_contact_states
-        rew_tracking_contacts_shaped_force = torch.zeros(self.num_envs, device=self.device)
-        for i in range(4):
-            # When desired_contact=0 (swing), penalize any contact force
-            rew_tracking_contacts_shaped_force += -(1 - desired_contact[:, i]) * (
-                1 - torch.exp(-1 * foot_forces[:, i] ** 2 / 100.0)
+        front_forces = foot_forces[:, self.fidx]
+        tracking_contact_penalty = torch.zeros(self.num_envs, device=self.device)
+        for i in range(2):  # Only front feet
+            tracking_contact_penalty += -(1 - self.desired_contact_states[:, self.fidx[i]]) * (
+                1 - torch.exp(-front_forces[:, i] ** 2 / 100.0)
             )
-        rew_tracking_contacts_shaped_force = rew_tracking_contacts_shaped_force / 4  # Average over 4 feet
+        rew_tracking_contacts = tracking_contact_penalty * is_stand * self.cfg.tracking_contacts_reward_scale / 2
         
+        # === 7. rew_torque_limits: Soft torque limit penalty ===
+        # -k_τ * Σ_j max(0, |τ_j| - τ_max * σ_s)
+        torque_excess = (torch.abs(self.computed_torques) - self.torque_limits_value * self.cfg.soft_torque_limit).clamp(min=0.0)
+        rew_torque_limits = torch.sum(torque_excess, dim=1) * self.cfg.torque_limits_reward_scale
+        
+        # === 8. rew_ang_vel_z: Angular velocity Z penalty (after t_c) ===
+        # -k_ω * ||ω_z||^2 * 1_{t>t_c}
+        rew_ang_vel_z = torch.square(self.robot.data.root_ang_vel_b[:, 2]) * condition_tc.float() * self.cfg.ang_vel_z_reward_scale
+        
+        # === 9. rew_action_rate: Action smoothness penalty ===
+        # -k_act * ||a_t - a_{t-1}||^2 (1st order)
+        # -k_act * ||(a_{t-2} - a_{t-1}) - (a_{t-1} - a_t)||^2 (2nd order)
+        action_diff_1 = torch.sum(torch.square(self._actions - self.last_actions[:, :, 0]), dim=1)
+        action_diff_2 = torch.sum(torch.square(
+            self._actions - 2 * self.last_actions[:, :, 0] + self.last_actions[:, :, 1]
+        ), dim=1)
+        rew_action_rate = (action_diff_1 + action_diff_2) * (self.cfg.action_scale ** 2) * self.cfg.action_rate_reward_scale
+        
+        # Update action history
+        self.last_actions = torch.roll(self.last_actions, 1, 2)
+        self.last_actions[:, :, 0] = self._actions[:]
+        
+        # === 10. rew_dof_vel: Joint velocity penalty ===
+        # -k_q̇ * ||q̇||^2
+        rew_dof_vel = torch.sum(torch.square(self.robot.data.joint_vel), dim=1) * self.cfg.dof_vel_reward_scale
+        
+        # =====================================================================
+        # TOTAL REWARD (line 1522)
+        # =====================================================================
         rewards = {
-            "track_lin_vel_xy_exp": lin_vel_error_mapped * self.cfg.lin_vel_reward_scale,
-            "track_ang_vel_z_exp": yaw_rate_error_mapped * self.cfg.yaw_rate_reward_scale,
-            "rew_action_rate": rew_action_rate * self.cfg.action_rate_reward_scale,
-            "raibert_heuristic": rew_raibert_heuristic * self.cfg.raibert_heuristic_reward_scale,
-            "orient": rew_orient * self.cfg.orient_reward_scale,
-            "lin_vel_z": rew_lin_vel_z * self.cfg.lin_vel_z_reward_scale,
-            "dof_vel": rew_dof_vel * self.cfg.dof_vel_reward_scale,
-            "ang_vel_xy": rew_ang_vel_xy * self.cfg.ang_vel_xy_reward_scale,
-            "feet_clearance": rew_feet_clearance * self.cfg.feet_clearance_reward_scale,
-            "tracking_contacts_shaped_force": rew_tracking_contacts_shaped_force * self.cfg.tracking_contacts_shaped_force_reward_scale,
+            "upright": rew_upright,
+            "lift_up": rew_lift_up,
+            "stand_air": rew_stand_air,
+            "lin_vel_xy": rew_lin_vel_xy,
+            "feet_clearance": rew_feet_clearance,
+            "tracking_contacts": rew_tracking_contacts,
+            "torque_limits": rew_torque_limits,
+            "ang_vel_z": rew_ang_vel_z,
+            "action_rate": rew_action_rate,
+            "dof_vel": rew_dof_vel,
         }
+        
         reward = torch.sum(torch.stack(list(rewards.values())), dim=0)
+        
+        # Clip reward to be non-negative (from reference line 1525-1527)
+        reward = torch.clip(reward, min=0.0)
+        
         # Logging
         for key, value in rewards.items():
             self._episode_sums[key] += value
+            
         return reward
 
     def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
+        """Compute bipedal termination conditions.
+        Reference: go2_bipedal.py lines 1894-1901
+        """
         time_out = self.episode_length_buf >= self.max_episode_length - 1
+        
+        allow_contact_steps = self.cfg.allow_contact_steps
+        allow_not_stand_steps = self.cfg.allow_not_stand_steps
+        
+        # === any_contacts: Base contact termination ===
         net_contact_forces = self._contact_sensor.data.net_forces_w_history
-        cstr_termination_contacts = torch.any(torch.max(torch.norm(net_contact_forces[:, :, self._base_id], dim=-1), dim=1)[0] > 1.0, dim=1)
-        cstr_upsidedown = self.robot.data.projected_gravity_b[:, 2] > 0
-
-        # === ADDED Part 3: Terminate if base is too low ===
+        cstr_base_contact = torch.any(
+            torch.max(torch.norm(net_contact_forces[:, :, self._base_id], dim=-1), dim=1)[0] > 1.0, 
+            dim=1
+        )
+        
+        # === stand_air_condition: Front feet too high during early phase ===
+        # Terminate if front feet > 6cm during early phase (3 < t <= allow_contact_steps)
+        foot_heights = self.foot_positions_w[:, :, 2]
+        front_heights = foot_heights[:, self.fidx]
+        stand_air_condition = torch.logical_and(
+            torch.logical_and(self.episode_length_buf > 3, self.episode_length_buf <= allow_contact_steps),
+            torch.any(front_heights > 0.06, dim=-1)
+        )
+        
+        # === abrupt_change_condition: Joint positions changed too fast ===
+        abrupt_change_condition = torch.logical_and(
+            torch.logical_and(self.episode_length_buf > 3, self.episode_length_buf <= allow_contact_steps),
+            torch.any(torch.abs(self.robot.data.joint_pos - self.last_dof_pos[:, :, 0]) > self.max_dof_change, dim=-1)
+        )
+        
+        # === position_protect: Joints near limits ===
+        joint_pos = self.robot.data.joint_pos
+        near_lower = joint_pos < (self.dof_pos_limits[:, 0] + 5.0 / 180.0 * np.pi)
+        near_upper = joint_pos > (self.dof_pos_limits[:, 1] - 5.0 / 180.0 * np.pi)
+        position_protect = torch.logical_and(
+            self.episode_length_buf > 3,
+            torch.any(torch.logical_or(near_lower, near_upper), dim=-1)
+        )
+        
+        # === not_stand: Not standing after allow_not_stand_steps ===
+        forward = math_utils.quat_rotate(self.robot.data.root_quat_w, self.forward_vec)
+        upright_vec = math_utils.quat_apply_yaw(self.robot.data.root_quat_w, self.upright_vec)
+        cosine_dist = torch.sum(forward * upright_vec, dim=-1) / (torch.norm(upright_vec, dim=-1) + 1e-6)
+        is_stand = cosine_dist > 0.9
+        not_stand = torch.logical_and(
+            self.episode_length_buf > allow_not_stand_steps,
+            ~is_stand
+        )
+        
+        # === cstr_base_height_min: Base height too low ===
         base_height = self.robot.data.root_pos_w[:, 2]
         cstr_base_height_min = base_height < self.cfg.base_height_min
         
-        died = cstr_termination_contacts | cstr_upsidedown | cstr_base_height_min
+        # === Combine all termination conditions (lines 1894-1901) ===
+        died = cstr_base_contact
+        died = died | abrupt_change_condition
+        died = died | position_protect
+        died = died | not_stand
+        died = died | stand_air_condition
+        died = died | cstr_base_height_min
+        
         return died, time_out
 
     def _reset_idx(self, env_ids: Sequence[int] | None):
@@ -248,13 +388,14 @@ class Rob6323Go2Env(DirectRLEnv):
             env_ids = self.robot._ALL_INDICES
         self.robot.reset(env_ids)
         super()._reset_idx(env_ids)
+        
         if len(env_ids) == self.num_envs:
-            # Spread out the resets to avoid spikes in training when many environments reset at a similar time
+            # Spread out resets to avoid training spikes
             self.episode_length_buf[:] = torch.randint_like(self.episode_length_buf, high=int(self.max_episode_length))
+        
         self._actions[env_ids] = 0.0
         self._previous_actions[env_ids] = 0.0
-        # Sample new commands
-        self._commands[env_ids] = torch.zeros_like(self._commands[env_ids]).uniform_(-1.0, 1.0)
+        
         # Reset robot state
         joint_pos = self.robot.data.default_joint_pos[env_ids]
         joint_vel = self.robot.data.default_joint_vel[env_ids]
@@ -263,6 +404,7 @@ class Rob6323Go2Env(DirectRLEnv):
         self.robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
         self.robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
         self.robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
+        
         # Logging
         extras = dict()
         for key in self._episode_sums.keys():
@@ -276,23 +418,20 @@ class Rob6323Go2Env(DirectRLEnv):
         extras["Episode_Termination/time_out"] = torch.count_nonzero(self.reset_time_outs[env_ids]).item()
         self.extras["log"].update(extras)
 
-        # === ADDED Part 1: Reset action history ===
-        self.last_actions[env_ids] = 0.
-
-        # === ADDED Part 4: Reset gait indices ===
+        # Reset action history
+        self.last_actions[env_ids] = 0.0
+        
+        # Reset DOF position history
+        self.last_dof_pos[env_ids] = self.robot.data.joint_pos[env_ids].unsqueeze(-1)
+        
+        # Reset gait indices
         self.gait_indices[env_ids] = 0
 
     def _set_debug_vis_impl(self, debug_vis: bool):
-        # set visibility of markers
-        # note: parent only deals with callbacks. not their visibility
         if debug_vis:
-            # create markers if necessary for the first time
             if not hasattr(self, "goal_vel_visualizer"):
-                # -- goal
                 self.goal_vel_visualizer = VisualizationMarkers(self.cfg.goal_vel_visualizer_cfg)
-                # -- current
                 self.current_vel_visualizer = VisualizationMarkers(self.cfg.current_vel_visualizer_cfg)
-            # set their visibility to true
             self.goal_vel_visualizer.set_visibility(True)
             self.current_vel_visualizer.set_visibility(True)
         else:
@@ -301,137 +440,84 @@ class Rob6323Go2Env(DirectRLEnv):
                 self.current_vel_visualizer.set_visibility(False)
 
     def _debug_vis_callback(self, event):
-        # check if robot is initialized
-        # note: this is needed in-case the robot is de-initialized. we can't access the data
         if not self.robot.is_initialized:
             return
-        # get marker location
-        # -- base state
+        # Visualize upright direction
         base_pos_w = self.robot.data.root_pos_w.clone()
         base_pos_w[:, 2] += 0.5
-        # -- resolve the scales and quaternions
-        vel_des_arrow_scale, vel_des_arrow_quat = self._resolve_xy_velocity_to_arrow(self._commands[:, :2])
-        vel_arrow_scale, vel_arrow_quat = self._resolve_xy_velocity_to_arrow(self.robot.data.root_lin_vel_b[:, :2])
-        # display markers
-        self.goal_vel_visualizer.visualize(base_pos_w, vel_des_arrow_quat, vel_des_arrow_scale)
+        # Show forward direction
+        forward = math_utils.quat_rotate(self.robot.data.root_quat_w, self.forward_vec)
+        vel_arrow_scale, vel_arrow_quat = self._resolve_xy_velocity_to_arrow(forward[:, :2])
+        # Show upright target
+        upright_vec = math_utils.quat_apply_yaw(self.robot.data.root_quat_w, self.upright_vec)
+        upright_scale, upright_quat = self._resolve_xy_velocity_to_arrow(upright_vec[:, :2])
+        
+        self.goal_vel_visualizer.visualize(base_pos_w, upright_quat, upright_scale)
         self.current_vel_visualizer.visualize(base_pos_w, vel_arrow_quat, vel_arrow_scale)
 
     def _resolve_xy_velocity_to_arrow(self, xy_velocity: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
-        """Converts the XY base velocity command to arrow direction rotation."""
-        # obtain default scale of the marker
+        """Converts XY vector to arrow direction rotation."""
         default_scale = self.goal_vel_visualizer.cfg.markers["arrow"].scale
-        # arrow-scale
         arrow_scale = torch.tensor(default_scale, device=self.device).repeat(xy_velocity.shape[0], 1)
         arrow_scale[:, 0] *= torch.linalg.norm(xy_velocity, dim=1) * 3.0
-        # arrow-direction
         heading_angle = torch.atan2(xy_velocity[:, 1], xy_velocity[:, 0])
         zeros = torch.zeros_like(heading_angle)
         arrow_quat = math_utils.quat_from_euler_xyz(zeros, zeros, heading_angle)
-        # convert everything back from base to world frame
         base_quat_w = self.robot.data.root_quat_w
         arrow_quat = math_utils.quat_mul(base_quat_w, arrow_quat)
-
         return arrow_scale, arrow_quat
 
-    # === ADDED Part 4: Foot positions property ===
     @property
     def foot_positions_w(self) -> torch.Tensor:
-        """Returns the feet positions in the world frame.
-        Shape: (num_envs, num_feet, 3)
-        """
+        """Returns feet positions in world frame. Shape: (num_envs, 4, 3)"""
         return self.robot.data.body_pos_w[:, self._feet_ids]
 
-    # === ADDED Part 4: Contact plan for gait ===
-    # Defines contact plan
     def _step_contact_targets(self):
-        frequencies = 3.
+        """Compute gait phase and desired contact states for front feet."""
+        frequencies = 3.0
         phases = 0.5
-        offsets = 0.
-        bounds = 0.
+        offsets = 0.0
+        bounds = 0.0
         durations = 0.5 * torch.ones((self.num_envs,), dtype=torch.float32, device=self.device)
+        
         self.gait_indices = torch.remainder(self.gait_indices + self.step_dt * frequencies, 1.0)
 
-        foot_indices = [self.gait_indices + phases + offsets + bounds,
-                        self.gait_indices + offsets,
-                        self.gait_indices + bounds,
-                        self.gait_indices + phases]
+        foot_indices = [
+            self.gait_indices + phases + offsets + bounds,  # FL
+            self.gait_indices + offsets,                     # FR
+            self.gait_indices + bounds,                      # RL
+            self.gait_indices + phases                       # RR
+        ]
 
-        self.foot_indices = torch.remainder(torch.cat([foot_indices[i].unsqueeze(1) for i in range(4)], dim=1), 1.0)
+        self.foot_indices = torch.remainder(
+            torch.cat([foot_indices[i].unsqueeze(1) for i in range(4)], dim=1), 
+            1.0
+        )
 
         for idxs in foot_indices:
             stance_idxs = torch.remainder(idxs, 1) < durations
             swing_idxs = torch.remainder(idxs, 1) > durations
-
             idxs[stance_idxs] = torch.remainder(idxs[stance_idxs], 1) * (0.5 / durations[stance_idxs])
             idxs[swing_idxs] = 0.5 + (torch.remainder(idxs[swing_idxs], 1) - durations[swing_idxs]) * (
-                        0.5 / (1 - durations[swing_idxs]))
+                0.5 / (1 - durations[swing_idxs]))
 
         self.clock_inputs[:, 0] = torch.sin(2 * np.pi * foot_indices[0])
         self.clock_inputs[:, 1] = torch.sin(2 * np.pi * foot_indices[1])
         self.clock_inputs[:, 2] = torch.sin(2 * np.pi * foot_indices[2])
         self.clock_inputs[:, 3] = torch.sin(2 * np.pi * foot_indices[3])
 
-        # von mises distribution
+        # Von Mises distribution for smooth contact transitions
         kappa = 0.07
-        smoothing_cdf_start = torch.distributions.normal.Normal(0, kappa).cdf  # (x) + torch.distributions.normal.Normal(1, kappa).cdf(x)) / 2
-
-        smoothing_multiplier_FL = (smoothing_cdf_start(torch.remainder(foot_indices[0], 1.0)) * (
-                1 - smoothing_cdf_start(torch.remainder(foot_indices[0], 1.0) - 0.5)) +
-                                    smoothing_cdf_start(torch.remainder(foot_indices[0], 1.0) - 1) * (
-                                            1 - smoothing_cdf_start(
-                                        torch.remainder(foot_indices[0], 1.0) - 0.5 - 1)))
-        smoothing_multiplier_FR = (smoothing_cdf_start(torch.remainder(foot_indices[1], 1.0)) * (
-                1 - smoothing_cdf_start(torch.remainder(foot_indices[1], 1.0) - 0.5)) +
-                                    smoothing_cdf_start(torch.remainder(foot_indices[1], 1.0) - 1) * (
-                                            1 - smoothing_cdf_start(
-                                        torch.remainder(foot_indices[1], 1.0) - 0.5 - 1)))
-        smoothing_multiplier_RL = (smoothing_cdf_start(torch.remainder(foot_indices[2], 1.0)) * (
-                1 - smoothing_cdf_start(torch.remainder(foot_indices[2], 1.0) - 0.5)) +
-                                    smoothing_cdf_start(torch.remainder(foot_indices[2], 1.0) - 1) * (
-                                            1 - smoothing_cdf_start(
-                                        torch.remainder(foot_indices[2], 1.0) - 0.5 - 1)))
-        smoothing_multiplier_RR = (smoothing_cdf_start(torch.remainder(foot_indices[3], 1.0)) * (
-                1 - smoothing_cdf_start(torch.remainder(foot_indices[3], 1.0) - 0.5)) +
-                                    smoothing_cdf_start(torch.remainder(foot_indices[3], 1.0) - 1) * (
-                                            1 - smoothing_cdf_start(
-                                        torch.remainder(foot_indices[3], 1.0) - 0.5 - 1)))
-
-        self.desired_contact_states[:, 0] = smoothing_multiplier_FL
-        self.desired_contact_states[:, 1] = smoothing_multiplier_FR
-        self.desired_contact_states[:, 2] = smoothing_multiplier_RL
-        self.desired_contact_states[:, 3] = smoothing_multiplier_RR
-
-    # === ADDED Part 4: Raibert heuristic reward ===
-    def _reward_raibert_heuristic(self):
-        cur_footsteps_translated = self.foot_positions_w - self.robot.data.root_pos_w.unsqueeze(1)
-        footsteps_in_body_frame = torch.zeros(self.num_envs, 4, 3, device=self.device)
-        for i in range(4):
-            footsteps_in_body_frame[:, i, :] = math_utils.quat_apply_yaw(math_utils.quat_conjugate(self.robot.data.root_quat_w), cur_footsteps_translated[:, i, :])
-
-        # nominal positions: [FR, FL, RR, RL]
-        desired_stance_width = 0.25
-        desired_ys_nom = torch.tensor([desired_stance_width / 2, -desired_stance_width / 2, desired_stance_width / 2, -desired_stance_width / 2], device=self.device).unsqueeze(0)
-
-        desired_stance_length = 0.45
-        desired_xs_nom = torch.tensor([desired_stance_length / 2,  desired_stance_length / 2, -desired_stance_length / 2, -desired_stance_length / 2], device=self.device).unsqueeze(0)
-
-        # raibert offsets
-        phases = torch.abs(1.0 - (self.foot_indices * 2.0)) * 1.0 - 0.5
-        frequencies = torch.tensor([3.0], device=self.device)
-        x_vel_des = self._commands[:, 0:1]
-        yaw_vel_des = self._commands[:, 2:3]
-        y_vel_des = yaw_vel_des * desired_stance_length / 2
-        desired_ys_offset = phases * y_vel_des * (0.5 / frequencies.unsqueeze(1))
-        desired_ys_offset[:, 2:4] *= -1
-        desired_xs_offset = phases * x_vel_des * (0.5 / frequencies.unsqueeze(1))
-
-        desired_ys_nom = desired_ys_nom + desired_ys_offset
-        desired_xs_nom = desired_xs_nom + desired_xs_offset
-
-        desired_footsteps_body_frame = torch.cat((desired_xs_nom.unsqueeze(2), desired_ys_nom.unsqueeze(2)), dim=2)
-
-        err_raibert_heuristic = torch.abs(desired_footsteps_body_frame - footsteps_in_body_frame[:, :, 0:2])
-
-        reward = torch.sum(torch.square(err_raibert_heuristic), dim=(1, 2))
-
-        return reward
\ No newline at end of file
+        smoothing_cdf = torch.distributions.normal.Normal(0, kappa).cdf
+
+        for i, foot_idx in enumerate(foot_indices):
+            smoothing_multiplier = (
+                smoothing_cdf(torch.remainder(foot_idx, 1.0)) * 
+                (1 - smoothing_cdf(torch.remainder(foot_idx, 1.0) - 0.5)) +
+                smoothing_cdf(torch.remainder(foot_idx, 1.0) - 1) * 
+                (1 - smoothing_cdf(torch.remainder(foot_idx, 1.0) - 0.5 - 1))
+            )
+            self.desired_contact_states[:, i] = smoothing_multiplier
+
+        # Update last_dof_pos for abrupt_change_condition
+        self.last_dof_pos[:, :, 0] = self.robot.data.joint_pos.clone()
diff --git a/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env_cfg.py b/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env_cfg.py
index e675050..80be86f 100644
--- a/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env_cfg.py
+++ b/source/rob6323_go2/rob6323_go2/tasks/direct/rob6323_go2/rob6323_go2_env_cfg.py
@@ -5,36 +5,50 @@
 
 from isaaclab_assets.robots.unitree import UNITREE_GO2_CFG
 
-import isaaclab.envs.mdp as mdp
 import isaaclab.sim as sim_utils
 from isaaclab.assets import ArticulationCfg
 from isaaclab.envs import DirectRLEnvCfg
 from isaaclab.scene import InteractiveSceneCfg
 from isaaclab.sim import SimulationCfg
 from isaaclab.utils import configclass
-from isaaclab.terrains import TerrainImporterCfg, TerrainGeneratorCfg
-from isaaclab.terrains.height_field import HfWaveTerrainCfg
+from isaaclab.terrains import TerrainImporterCfg
 from isaaclab.sensors import ContactSensorCfg
 from isaaclab.markers import VisualizationMarkersCfg
-from isaaclab.markers.config import BLUE_ARROW_X_MARKER_CFG, FRAME_MARKER_CFG, GREEN_ARROW_X_MARKER_CFG
-
-# === ADDED Part 2: PD controller ===
-# add this import:
+from isaaclab.markers.config import BLUE_ARROW_X_MARKER_CFG, GREEN_ARROW_X_MARKER_CFG
 from isaaclab.actuators import ImplicitActuatorCfg
 
+
 @configclass
 class Rob6323Go2EnvCfg(DirectRLEnvCfg):
+    # === BIPEDAL STANDING CONFIGURATION ===
+    
     # env
     decimation = 4
     episode_length_s = 20.0
-    # - spaces definition
+    
+    # Spaces definition
+    # Observation: 3 (lin_vel) + 3 (ang_vel) + 3 (gravity) + 3 (upright_vec) + 12 (joint_pos) + 12 (joint_vel) + 12 (actions) + 4 (clock) = 52
     action_scale = 0.25
     action_space = 12
-    observation_space = 48 + 4  # === MODIFIED Part 4: Added 4 for clock inputs ===
+    observation_space = 52
     state_space = 0
     debug_vis = True
-    # === ADDED Part 3: Early termination ===
-    base_height_min = 0.20  # Terminate if base is lower than 20cm
+
+    # === BIPEDAL: Phase control thresholds ===
+    allow_contact_steps = 30   # t_c: when to start applying standing rewards
+    allow_not_stand_steps = 60  # when to require standing posture
+    
+    # === BIPEDAL: Lift-up height thresholds ===
+    lift_up_h_min = 0.27  # H_min for lift-up reward
+    lift_up_h_max = 0.55  # H_max for lift-up reward
+    
+    # === BIPEDAL: Base height termination threshold ===
+    base_height_min = 0.15  # Terminate if base is lower than 15cm
+
+    # === BIPEDAL: Scaling factors for no-velocity reward ===
+    scale_factor_low = 0.33
+    scale_factor_high = 0.45
+    lin_vel_delta = 0.25  # δ for exp kernel
 
     # simulation
     sim: SimulationCfg = SimulationCfg(
@@ -48,31 +62,11 @@ class Rob6323Go2EnvCfg(DirectRLEnvCfg):
             restitution=0.0,
         ),
     )
-    # === MODIFIED: Wave terrain for locomotion challenge ===
-    # Reference settings from Go2Terrain.yaml: 8m x 8m tiles, 10 levels, 20 terrains
-    # Robot length ~0.7m, peak-to-peak wavelength >= 1.4m (2x robot length)
-    # With 8m terrain and 5 waves: wavelength = 8/5 = 1.6m per wave (meets requirement)
+    
+    # Flat terrain for bipedal standing (simpler than walking)
     terrain = TerrainImporterCfg(
         prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=TerrainGeneratorCfg(
-            size=(40.0, 40.0),  # 8m x 8m per tile (from reference)
-            border_width=20.0,  # Flat spawn border
-            num_rows=1,  # numLevels from reference
-            num_cols=1,  # numTerrains from reference
-            horizontal_scale=0.1,  # Resolution: 10cm per height sample
-            vertical_scale=0.005,  # Height scale factor
-            slope_threshold=0.75,
-            use_cache=False,
-            sub_terrains={
-                "waves": HfWaveTerrainCfg(
-                    proportion=1.0,  # 100% wave terrain
-                    amplitude_range=(0.2, 0.2),  # Wave height: 2-6cm amplitude
-                    num_waves=2,  # 5 waves per 8m = 1.6m wavelength (> 1.4m requirement)
-                    border_width=0.5,
-                ),
-            },
-        ),
+        terrain_type="plane",
         collision_group=-1,
         physics_material=sim_utils.RigidBodyMaterialCfg(
             friction_combine_mode="multiply",
@@ -83,26 +77,23 @@ class Rob6323Go2EnvCfg(DirectRLEnvCfg):
         ),
         debug_vis=False,
     )
-    # === END MODIFIED ===
-
-    # robot(s)
-    # robot_cfg: ArticulationCfg = UNITREE_GO2_CFG.replace(prim_path="/World/envs/env_.*/Robot")
 
-    # === ADDED Part 2: PD control gains ===
+    # === PD control gains (reused from quadruped) ===
     Kp = 20.0  # Proportional gain
     Kd = 0.5   # Derivative gain
-    torque_limits = 100.0  # Max torque
+    torque_limits = 23.5  # Max torque (from Go2 specs)
+    
+    # === BIPEDAL: Soft torque limit for reward ===
+    soft_torque_limit = 0.5  # σ_s: fraction of max torque before penalty
 
-    # === MODIFIED Part 2: Disable implicit actuator PD, use custom PD ===
-    # Update robot_cfg
+    # Robot configuration with custom PD
     robot_cfg: ArticulationCfg = UNITREE_GO2_CFG.replace(prim_path="/World/envs/env_.*/Robot")
-    # "base_legs" is an arbitrary key we use to group these actuators
     robot_cfg.actuators["base_legs"] = ImplicitActuatorCfg(
         joint_names_expr=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"],
         effort_limit=23.5,
         velocity_limit=30.0,
-        stiffness=0.0,  # CRITICAL: Set to 0 to disable implicit P-gain
-        damping=0.0,    # CRITICAL: Set to 0 to disable implicit D-gain
+        stiffness=0.0,  # Disable implicit P-gain (use custom PD)
+        damping=0.0,    # Disable implicit D-gain (use custom PD)
     )
     
     # scene
@@ -110,34 +101,30 @@ class Rob6323Go2EnvCfg(DirectRLEnvCfg):
     contact_sensor: ContactSensorCfg = ContactSensorCfg(
         prim_path="/World/envs/env_.*/Robot/.*", history_length=3, update_period=0.005, track_air_time=True
     )
+    
+    # Visualization markers
     goal_vel_visualizer_cfg: VisualizationMarkersCfg = GREEN_ARROW_X_MARKER_CFG.replace(
         prim_path="/Visuals/Command/velocity_goal"
     )
-    """The configuration for the goal velocity visualization marker. Defaults to GREEN_ARROW_X_MARKER_CFG."""
-
     current_vel_visualizer_cfg: VisualizationMarkersCfg = BLUE_ARROW_X_MARKER_CFG.replace(
         prim_path="/Visuals/Command/velocity_current"
     )
-    """The configuration for the current velocity visualization marker. Defaults to BLUE_ARROW_X_MARKER_CFG."""
-
-    # Set the scale of the visualization markers to (0.5, 0.5, 0.5)
     goal_vel_visualizer_cfg.markers["arrow"].scale = (0.5, 0.5, 0.5)
     current_vel_visualizer_cfg.markers["arrow"].scale = (0.5, 0.5, 0.5)
 
-    # reward scales
-    lin_vel_reward_scale = 1.0
-    yaw_rate_reward_scale = 0.5
-    # === ADDED Part 1: Action rate penalty ===
-    action_rate_reward_scale = -0.1
-    # === ADDED Part 4: Raibert heuristic ===
-    raibert_heuristic_reward_scale = -10.0
-    # === ADDED: Part 5 rewards ===
-    orient_reward_scale = 0.0  # Penalize non-flat orientation
-    lin_vel_z_reward_scale = 0.0  # Penalize vertical bouncing
-    dof_vel_reward_scale = -0.0001  # Penalize high joint velocities
-    ang_vel_xy_reward_scale = -0.001  # Penalize roll/pitch angular velocity
-    # === ADDED: Part 6: Foot clearance ===
-    feet_clearance_reward_scale = -30.0  # Penalize low foot height during swing
-    # === ADDED: Part 6: Tracking contacts shaped force ===
-    tracking_contacts_shaped_force_reward_scale = 4.0  # Reward matching gait contact plan
-
+    # === BIPEDAL REWARD SCALES (from Paper Table 3) ===
+    # Rewards (positive = good)
+    upright_reward_scale = 1.0           # k_up: Upright posture reward
+    lift_up_reward_scale = 0.5           # k_lift: Height increase reward
+    stand_air_reward_scale = 5.0         # k_air_rew: Reward rear feet grounded early
+    lin_vel_reward_scale = 1.0           # k_lin: No velocity reward when standing
+    
+    # Penalties (negative = bad)
+    stand_air_penalty_scale = -40.0      # k_air_pen: Penalize front feet > 6cm early
+    vel_penalty_scale = -0.4             # k_vel_pen: Velocity penalty when standing
+    ang_vel_z_reward_scale = -0.1        # k_ω: Angular velocity Z penalty
+    action_rate_reward_scale = -0.03     # k_act: Action rate penalty (1st + 2nd order)
+    dof_vel_reward_scale = -0.0001       # k_q̇: Joint velocity penalty
+    torque_limits_reward_scale = -0.01   # k_τ: Torque limits penalty
+    feet_clearance_reward_scale = -300.0  # k_clr: Front feet clearance penalty
+    tracking_contacts_reward_scale = -1.0   # k_cfs: Contact tracking (front feet only)